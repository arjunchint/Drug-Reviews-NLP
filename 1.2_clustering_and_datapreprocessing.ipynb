{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     C:\\Users\\10252946\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10252946\\Documents\\Education\\GATech\\BigDataForHealth\\Projects\\venv\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\10252946\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\10252946\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\10252946\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "random.seed(1)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.downloader.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "\n",
    "import string\n",
    "from time import time\n",
    "\n",
    "from bs4 import BeautifulSoup  \n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: C:\\Users\\10252946\\Documents\\Education\\GATech\\BigDataForHealth\\Projects\nPath of the raw data: C:\\Users\\10252946\\Documents\\Education\\GATech\\BigDataForHealth\\Projects/drugsCom\nFiles in the drugCom directory: ['drug-enforcement-0001-of-0001.json', 'drug-enforcement-0001-of-0001.json.zip', 'drugsComTest_raw.tsv', 'drugsComTrain_raw.tsv', 'drugs_recalls_fda.tsv', 'FDA_Drug_Recalls_2012_to_2016_by_Recall_Class.csv', 'fda_med_devices_recalls_2014_to_2017.tsv']\n"
     ]
    }
   ],
   "source": [
    "cur_dir = os.getcwd()\n",
    "print(\"Current directory: {}\".format(cur_dir))\n",
    "# list files in the current directory\n",
    "#print(\"Files in the current directory: {}\".format(os.listdir(cur_dir)))\n",
    "data_path = cur_dir+\"/drugsCom\"\n",
    "train_path = data_path + \"/drugsComTrain_raw.tsv\"\n",
    "test_path = data_path + \"/drugsComTest_raw.tsv\"\n",
    "#rawdata_path = cur_dir+\"/drugsCom\"\n",
    "#metadata_path = cur_dir+\"/meta_Health_and_Personal_Care.json.gz\"\n",
    "print(\"Path of the raw data: {}\".format(data_path))\n",
    "print(\"Files in the drugCom directory: {}\".format(os.listdir(data_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_table(train_path, sep='\\t')\n",
    "test = pd.read_table(test_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>206461</td>\n",
       "      <td>Valsartan</td>\n",
       "      <td>Left Ventricular Dysfunction</td>\n",
       "      <td>\"It has no side effect, I take it in combinati...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>May 20, 2012</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95260</td>\n",
       "      <td>Guanfacine</td>\n",
       "      <td>ADHD</td>\n",
       "      <td>\"My son is halfway through his fourth week of ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>April 27, 2010</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92703</td>\n",
       "      <td>Lybrel</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>December 14, 2009</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>138000</td>\n",
       "      <td>Ortho Evra</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"This is my first time using any form of birth...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>November 3, 2015</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35696</td>\n",
       "      <td>Buprenorphine / naloxone</td>\n",
       "      <td>Opiate Dependence</td>\n",
       "      <td>\"Suboxone has completely turned my life around...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>November 27, 2016</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>206461</td>\n",
       "      <td>Valsartan</td>\n",
       "      <td>Left Ventricular Dysfunction</td>\n",
       "      <td>\"It has no side effect, I take it in combinati...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>May 20, 2012</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95260</td>\n",
       "      <td>Guanfacine</td>\n",
       "      <td>ADHD</td>\n",
       "      <td>\"My son is halfway through his fourth week of ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>April 27, 2010</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92703</td>\n",
       "      <td>Lybrel</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>December 14, 2009</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>138000</td>\n",
       "      <td>Ortho Evra</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"This is my first time using any form of birth...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>November 3, 2015</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35696</td>\n",
       "      <td>Buprenorphine / naloxone</td>\n",
       "      <td>Opiate Dependence</td>\n",
       "      <td>\"Suboxone has completely turned my life around...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>November 27, 2016</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(161297, 7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3436"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['drugName'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = train['review'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_train, documents_test = train_test_split(documents, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data set\n",
    "\n",
    "verb_exp = ['VB', 'VBZ', 'VBP', 'VBD','VBN','VBG']\n",
    "#porter_stemmer = PorterStemmer()\n",
    "def clean_text(row):\n",
    "    soup = BeautifulSoup(row, 'html.parser')\n",
    "    #remove code\n",
    "    for tag in soup.find_all('code'):\n",
    "        tag.replaceWith(' ')\n",
    "        \n",
    "    raw = soup.get_text()\n",
    "    #remove link\n",
    "    raw_no_link = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', raw)\n",
    "    #remove email\n",
    "    no_link_email = re.sub(r'[\\w\\.-]+@[\\w\\.-]+[\\.][com|org|ch|uk]{2,3}', \"\", raw_no_link)\n",
    "    #remove whitespace\n",
    "    tab_text = '\\t\\n\\r\\x0b\\x0c'\n",
    "    no_link_email_space = \"\".join([ch for ch in no_link_email if ch not in set(tab_text)])\n",
    "    #remove fomula\n",
    "    reg = '(\\$.+?\\$)|((\\\\\\\\begin\\{.+?\\})(.+?)(\\\\\\\\end\\{(.+?)\\}))'\n",
    "    raw = re.sub(reg, \"\", no_link_email_space, flags=re.IGNORECASE)   \n",
    "    return(raw)\n",
    "\n",
    "def lem_stop(row):\n",
    "    row = row.lower()\n",
    "    #remove numbers\n",
    "    raw = re.sub('[0-9]+?', ' ', row) \n",
    "    # remove punctuation\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    raw = regex.sub(' ', raw)\n",
    "    #clean out the characters left out after the above step, like we’re, I’m, It’s, i.e.\n",
    "    raw = re.sub('( s )|( re )|( m )|( i e )',' ',raw) \n",
    "    # lementize\n",
    "    row_t = TextBlob(raw)\n",
    "    raw = []\n",
    "    for word, pos in row_t.tags:\n",
    "        if pos in verb_exp:\n",
    "            word = Word(word)\n",
    "            word = word.lemmatize(\"v\")\n",
    "        else:\n",
    "            word = Word(word)\n",
    "            word = word.lemmatize()\n",
    "        raw.append(word)\n",
    "    clean = ' '.join(raw)      \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # remove stop words\n",
    "    cleaned_text = \" \".join([word for word in word_tokenize(clean) if word not in stop_words])      \n",
    "    return(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the clean review\n",
    "review_rm = train['review'].map(lambda i: clean_text(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_clean = review_rm.map(lambda i: lem_stop(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_ana(i):\n",
    "    i_tr = TextBlob(i)\n",
    "    return(i_tr.sentiment[0], i_tr.sentiment[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentiment = review_clean.map(lambda i: sentiment_ana(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                          (0.0, 0.0)\n1                    (0.18802083333333336, 0.4828125)\n2          (0.13848484848484852, 0.32747474747474753)\n3           (0.35714285714285715, 0.6071428571428571)\n4           (0.17233333333333328, 0.3896666666666667)\n5          (0.10208333333333333, 0.28541666666666665)\n6           (0.1111111111111111, 0.16666666666666666)\n7                      (0.059166666666666666, 0.2825)\n8                           (0.2, 0.5166666666666666)\n9                      (0.04375000000000002, 0.34375)\n10           (0.21000000000000002, 0.653888888888889)\n11                       (0.2525, 0.4583333333333333)\n12          (-0.17777777777777773, 0.611111111111111)\n13        (-0.036574074074074064, 0.5416666666666665)\n14         (0.05286796536796535, 0.49983766233766236)\n15          (0.19722222222222227, 0.5981481481481482)\n16        (-0.038636363636363635, 0.4212121212121212)\n17         (0.30833333333333335, 0.47222222222222215)\n18           (0.1791666666666667, 0.5923611111111111)\n19         (-0.07346938775510206, 0.5331632653061225)\n20                       (0.5025, 0.5974999999999999)\n21         (0.003803030303030283, 0.5781666666666666)\n22          (0.30000000000000004, 0.7814814814814816)\n23         (0.12100550964187327, 0.37121212121212116)\n24                                         (0.0, 0.0)\n25         (0.31333333333333335, 0.44000000000000006)\n26            (0.1111111111111111, 0.461111111111111)\n27            (0.09664694280078895, 0.43801775147929)\n28                         (0.6, 0.43333333333333335)\n29          (0.027901785714285733, 0.548735119047619)\n                             ...                     \n161267                              (-0.2775, 0.6525)\n161268     (0.036111111111111115, 0.3138888888888889)\n161269      (0.10677083333333334, 0.4166666666666667)\n161270                  (-0.07840909090909087, 0.375)\n161271    (-0.033333333333333326, 0.3598484848484848)\n161272      (0.10092592592592592, 0.2592592592592593)\n161273     (0.04722222222222222, 0.42777777777777776)\n161274     (-0.19866071428571427, 0.5575892857142857)\n161275                              (-0.38125, 0.675)\n161276                              (0.15625, 0.3875)\n161277                                    (-0.5, 1.0)\n161278     (0.11666666666666665, 0.28333333333333327)\n161279                                  (0.125, 0.65)\n161280       (0.2592261904761905, 0.6261904761904763)\n161281     (-0.007621082621082631, 0.478988603988604)\n161282      (0.05805555555555554, 0.3572222222222222)\n161283    (-0.015833333333333303, 0.5563888888888889)\n161284                  (0.12125, 0.4241666666666667)\n161285                                     (0.0, 0.0)\n161286    (-0.10069444444444446, 0.44999999999999996)\n161287    (-0.09230769230769229, 0.46474358974358976)\n161288        (0.196969696969697, 0.7257575757575757)\n161289                                     (0.0, 0.0)\n161290      (-0.3833333333333333, 0.8888888888888888)\n161291    (-0.03819444444444444, 0.20309829059829057)\n161292       (0.2291666666666667, 0.3979166666666667)\n161293     (-0.31666666666666665, 0.9166666666666666)\n161294     (-0.3857142857142856, 0.40476190476190477)\n161295       (0.22283549783549786, 0.511038961038961)\n161296      (0.10416666666666667, 0.5333333333333333)\nName: review, Length: 161297, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_pol, senti_sub = [list(t) for t in zip(*sentiment)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(161297,)\n"
     ]
    }
   ],
   "source": [
    "print(review_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Sentimental_Polarity'] = senti_pol\n",
    "train['Sentimental_Subjectivity'] = senti_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(161297, 9)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "      <th>Sentimental_Polarity</th>\n",
       "      <th>Sentimental_Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>206461</td>\n",
       "      <td>Valsartan</td>\n",
       "      <td>Left Ventricular Dysfunction</td>\n",
       "      <td>\"It has no side effect, I take it in combinati...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>May 20, 2012</td>\n",
       "      <td>27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95260</td>\n",
       "      <td>Guanfacine</td>\n",
       "      <td>ADHD</td>\n",
       "      <td>\"My son is halfway through his fourth week of ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>April 27, 2010</td>\n",
       "      <td>192</td>\n",
       "      <td>0.188021</td>\n",
       "      <td>0.482812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92703</td>\n",
       "      <td>Lybrel</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>December 14, 2009</td>\n",
       "      <td>17</td>\n",
       "      <td>0.138485</td>\n",
       "      <td>0.327475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>138000</td>\n",
       "      <td>Ortho Evra</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"This is my first time using any form of birth...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>November 3, 2015</td>\n",
       "      <td>10</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.607143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35696</td>\n",
       "      <td>Buprenorphine / naloxone</td>\n",
       "      <td>Opiate Dependence</td>\n",
       "      <td>\"Suboxone has completely turned my life around...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>November 27, 2016</td>\n",
       "      <td>37</td>\n",
       "      <td>0.172333</td>\n",
       "      <td>0.389667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "      <th>Sentimental_Polarity</th>\n",
       "      <th>Sentimental_Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>206461</td>\n",
       "      <td>Valsartan</td>\n",
       "      <td>Left Ventricular Dysfunction</td>\n",
       "      <td>\"It has no side effect, I take it in combinati...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>May 20, 2012</td>\n",
       "      <td>27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95260</td>\n",
       "      <td>Guanfacine</td>\n",
       "      <td>ADHD</td>\n",
       "      <td>\"My son is halfway through his fourth week of ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>April 27, 2010</td>\n",
       "      <td>192</td>\n",
       "      <td>0.188021</td>\n",
       "      <td>0.482812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92703</td>\n",
       "      <td>Lybrel</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>December 14, 2009</td>\n",
       "      <td>17</td>\n",
       "      <td>0.138485</td>\n",
       "      <td>0.327475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>138000</td>\n",
       "      <td>Ortho Evra</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"This is my first time using any form of birth...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>November 3, 2015</td>\n",
       "      <td>10</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.607143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35696</td>\n",
       "      <td>Buprenorphine / naloxone</td>\n",
       "      <td>Opiate Dependence</td>\n",
       "      <td>\"Suboxone has completely turned my life around...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>November 27, 2016</td>\n",
       "      <td>37</td>\n",
       "      <td>0.172333</td>\n",
       "      <td>0.389667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add the recall by combining data from fda dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()\n",
    "data_path = cur_dir+\"/drugsCom\"\n",
    "fda_2016_path = data_path + \"/drugs_recalls_fda.tsv\"\n",
    "fda_2014_path = data_path + \"/FDA_Drug_Recalls_2012_to_2016_by_Recall_Class.csv\"\n",
    "fda_2016_onwards = pd.read_csv(fda_2016_path, parse_dates = [0], sep='\\t')\n",
    "fda_2012_to_2016 = pd.read_csv(fda_2014_path, parse_dates=[0], sep=',')\n",
    "\n",
    "fda_2012_to_2016['Date'] = fda_2012_to_2016['Recall Initiation Date']\n",
    "fda_2012_to_2016['Brand'] = fda_2012_to_2016['Recalling Firm']\n",
    "fda_2012_to_2016['Reason / Problem'] = fda_2012_to_2016['Reason for Recall']\n",
    "fda_2012_to_2016['Company'] = fda_2012_to_2016['Recalling Firm']\n",
    "\n",
    "all_fda_data = pd.concat([fda_2016_onwards, \\\n",
    "                              fda_2012_to_2016[['Date', 'Brand', \\\n",
    "                                                'Product Description', 'Reason / Problem', 'Company']]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Guanfacine'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_unique_drugs = train['drugName'].unique()\n",
    "len(train_unique_drugs)\n",
    "train_unique_drugs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3529\n"
     ]
    }
   ],
   "source": [
    "total_products_recalled = 0\n",
    "for online_drug in train_unique_drugs:\n",
    "    total_products_recalled += all_fda_data['Product Description'].str.contains(online_drug, regex=False).sum()\n",
    "\n",
    "print(total_products_recalled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3436\n"
     ]
    }
   ],
   "source": [
    "print (train_unique_drugs.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import dateutil.parser\n",
    "recallstatus = []\n",
    "for drugName, dateValue in zip(train['drugName'],train['date']):\n",
    "    recalled_drugs = all_fda_data['Product Description'].str.contains(drugName.split(' / ')[0], regex=False)\n",
    "    if any(recalled_drugs):\n",
    "        recallDateTime = str(all_fda_data[recalled_drugs]['Date'].iloc[0])\n",
    "        recallDateTime = recallDateTime.replace(\"/\", \"-\")\n",
    "        if dateutil.parser.parse(dateValue) < \\\n",
    "                dateutil.parser.parse(recallDateTime):\n",
    "            recallstatus.append(True)\n",
    "        else:\n",
    "            recallstatus.append(False)\n",
    "    else:\n",
    "        recallstatus.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train['recall_status'] = recallstatus\n",
    "# train.drop('recall_status', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(161297, 10)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31256, 10)\n"
     ]
    }
   ],
   "source": [
    "train_recall_df = train.loc[train['recall_status'] == True]\n",
    "print(train_recall_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130041, 10)\n"
     ]
    }
   ],
   "source": [
    "train_nonrecall_df = train.loc[train['recall_status'] == False]\n",
    "print(train_nonrecall_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_recall_df.to_csv(\"drugsTrainingDataRecallOnlyWithFeatures\", sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nonrecall_df.to_csv(\"drugsTrainingDataNonRecallOnlyWithFeatures\", sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66880, 10)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[(train['Sentimental_Polarity'] < 0.02)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31256, 10)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_recall_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13387, 10)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_recall_df.loc[(train_recall_df['Sentimental_Polarity'] < 0.05)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11230, 10)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_recall_df.loc[(train_recall_df['Sentimental_Polarity'] > 0.05) & (train_recall_df['usefulCount'] > 20)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130041, 10)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nonrecall_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55513, 10)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nonrecall_df.loc[(train_nonrecall_df['Sentimental_Polarity'] > 0.09)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74528, 10)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nonrecall_df.loc[(train_nonrecall_df['Sentimental_Polarity'] <= 0.09)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"drugsTrainingDataWithFeatures\", sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run sentimental analysis on Test Data i.e. drugsComTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the clean review\n",
    "review_rm_test = test['review'].map(lambda i: clean_text(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_clean_test = review_rm_test.map(lambda i: lem_stop(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_ana(i):\n",
    "    i_tr = TextBlob(i)\n",
    "    return(i_tr.sentiment[0], i_tr.sentiment[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_test = review_clean_test.map(lambda i: sentiment_ana(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_pol_test, senti_sub_test = [list(t) for t in zip(*sentiment_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Sentimental_Polarity'] = senti_pol_test\n",
    "test['Sentimental_Subjectivity'] = senti_sub_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53766, 9)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53766\n"
     ]
    }
   ],
   "source": [
    "import dateutil.parser\n",
    "test_recallstatus = []\n",
    "for drugName, dateValue in zip(test['drugName'],test['date']):\n",
    "    recalled_drugs = all_fda_data['Product Description'].str.contains(drugName.split(' / ')[0], regex=False)\n",
    "    if any(recalled_drugs):\n",
    "        recallDateTime = str(all_fda_data[recalled_drugs]['Date'].iloc[0])\n",
    "        recallDateTime = recallDateTime.replace(\"/\", \"-\")\n",
    "        if dateutil.parser.parse(dateValue) < \\\n",
    "                dateutil.parser.parse(recallDateTime):\n",
    "            test_recallstatus.append(True)\n",
    "        else:\n",
    "            test_recallstatus.append(False)\n",
    "    else:\n",
    "        test_recallstatus.append(False)\n",
    "\n",
    "print(len(test_recallstatus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# series_data = pd.Series(test_recallstatus)\n",
    "test['recall_status'] = test_recallstatus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53766, 10)\n"
     ]
    }
   ],
   "source": [
    "# test['recall_status'] = test_recallstatus\n",
    "# test.drop('recall_status', axis=1, inplace=True)\n",
    "print(test.shape)\n",
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31256, 10)\n"
     ]
    }
   ],
   "source": [
    "test_recall_df = train.loc[train['recall_status'] == True]\n",
    "print(test_recall_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130041, 10)\n"
     ]
    }
   ],
   "source": [
    "test_nonrecall_df = train.loc[train['recall_status'] == False]\n",
    "print(test_nonrecall_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recall_df.to_csv(\"drugsTestDataRecallOnlyWithFeatures\", sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nonrecall_df.to_csv(\"drugsTestDataNonRecallOnlyWithFeatures\", sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"drugsTestingDataWithFeatures\", sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     C:\\Users\\10252946\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def VaderAnalysis(inputstring):\n",
    "    all_sent = sent_tokenize(inputstring)\n",
    "    totalPositiveCount = 0\n",
    "    totalNegativeCount = 0\n",
    "    totalNeutralCount = 0\n",
    "    totalCompountCount = 0\n",
    "    totalSentenceCount = 0 \n",
    "    averagePositiveScore = 0\n",
    "    averageNegativeScore = 0\n",
    "    averageNeutralScroe = 0\n",
    "    averageCompountCount = 0\n",
    "    for i in range(len(all_sent)):\n",
    "        sentence = all_sent[i]\n",
    "        snt = analyser.polarity_scores(sentence)\n",
    "        totalPositiveCount = totalPositiveCount + snt['pos']\n",
    "        totalNegativeCount = totalNegativeCount + snt['neg']\n",
    "        totalNeutralCount = totalNeutralCount + snt['neu']\n",
    "        totalCompountCount = totalCompountCount + snt['compound']\n",
    "        totalSentenceCount = totalSentenceCount + 1\n",
    "    if totalSentenceCount > 0:\n",
    "        averagePositiveScore = totalPositiveCount/totalSentenceCount\n",
    "        averageNegativeScore = totalNegativeCount/totalSentenceCount\n",
    "        averageNeutralScroe = totalNeutralCount/totalSentenceCount\n",
    "        averageCompountCount = totalCompountCount/totalSentenceCount\n",
    "    return (averageNegativeScore, averageNeutralScroe, averagePositiveScore, averageCompountCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data set\n",
    "\n",
    "verb_exp = ['VB', 'VBZ', 'VBP', 'VBD','VBN','VBG']\n",
    "#porter_stemmer = PorterStemmer()\n",
    "def clean_text_va(row):\n",
    "    soup = BeautifulSoup(row, 'html.parser')\n",
    "    #remove code\n",
    "    for tag in soup.find_all('code'):\n",
    "        tag.replaceWith(' ')\n",
    "        \n",
    "    raw = soup.get_text()\n",
    "    #remove link\n",
    "    raw_no_link = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', raw)\n",
    "    #remove email\n",
    "    no_link_email = re.sub(r'[\\w\\.-]+@[\\w\\.-]+[\\.][com|org|ch|uk]{2,3}', \"\", raw_no_link)\n",
    "    #remove whitespace\n",
    "    tab_text = '\\t\\n\\r\\x0b\\x0c'\n",
    "    no_link_email_space = \"\".join([ch for ch in no_link_email if ch not in set(tab_text)])\n",
    "    #remove fomula\n",
    "    reg = '(\\$.+?\\$)|((\\\\\\\\begin\\{.+?\\})(.+?)(\\\\\\\\end\\{(.+?)\\}))'\n",
    "    raw = re.sub(reg, \"\", no_link_email_space, flags=re.IGNORECASE)   \n",
    "    return(raw)\n",
    "\n",
    "def lem_stop_va(row):\n",
    "    row = row.lower()\n",
    "    #remove numbers\n",
    "    raw = re.sub('[0-9]+?', ' ', row) \n",
    "    # # remove punctuation\n",
    "    # regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # raw = regex.sub(' ', raw)\n",
    "    #clean out the characters left out after the above step, like we’re, I’m, It’s, i.e.\n",
    "    raw = re.sub('( s )|( re )|( m )|( i e )',' ',raw) \n",
    "    # lementize\n",
    "    row_t = TextBlob(raw)\n",
    "    raw = []\n",
    "    for word, pos in row_t.tags:\n",
    "        if pos in verb_exp:\n",
    "            word = Word(word)\n",
    "            word = word.lemmatize(\"v\")\n",
    "        else:\n",
    "            word = Word(word)\n",
    "            word = word.lemmatize()\n",
    "        raw.append(word)\n",
    "    clean = ' '.join(raw)      \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # remove stop words\n",
    "    cleaned_text = \" \".join([word for word in word_tokenize(clean) if word not in stop_words])      \n",
    "    return(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_va_clean = train['review'].map(lambda i: clean_text_va(i))\n",
    "review_va_stop = review_va_clean.map(lambda i: lem_stop_va(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaderAnalysisData = review_va_stop.map(lambda i: VaderAnalysis(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_neg, senti_neu, senti_pos, senti_compount = [list(t) for t in zip(*vaderAnalysisData)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['vader_neg'] = senti_neg\n",
    "train['vader_neu'] = senti_neu\n",
    "train['vader_pos'] = senti_pos\n",
    "train['vader_comp'] = senti_compount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(161297, 14)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31256, 14)\n"
     ]
    }
   ],
   "source": [
    "train_recall_df = train.loc[train['recall_status'] == True]\n",
    "print(train_recall_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130041, 14)\n"
     ]
    }
   ],
   "source": [
    "train_nonrecall_df = train.loc[train['recall_status'] == False]\n",
    "print(train_nonrecall_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_recall_df.to_csv(\"drugsTrainingDataRecallOnlyWithFeatures\", sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nonrecall_df.to_csv(\"drugsTrainingDataNonRecallOnlyWithFeatures\", sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"drugsTrainingDataWithFeatures\", sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_va_clean_test = test['review'].map(lambda i: clean_text_va(i))\n",
    "review_va_stop_test = review_va_clean_test.map(lambda i: lem_stop_va(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaderAnalysisData_test = review_va_stop_test.map(lambda i: VaderAnalysis(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_neg_t, senti_neu_t, senti_pos_t, senti_compound_t = [list(t) for t in zip(*vaderAnalysisData_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['vader_neg'] = senti_neg_t\n",
    "test['vader_neu'] = senti_neu_t\n",
    "test['vader_pos'] = senti_pos_t\n",
    "test['vader_comp'] = senti_compound_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Stanford sentimental analysis feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "def StanfordAnalysis(inputstring):\n",
    "    nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "    totalPositiveCount = 0\n",
    "    totalNegativeCount = 0\n",
    "    totalNeutralCount = 0\n",
    "    totalSentenceCount = 0 \n",
    "    averagePositiveScore = 0\n",
    "    averageNegativeScore = 0\n",
    "    averageNeutralScroe = 0\n",
    "    \n",
    "    res = nlp.annotate(inputstring,\n",
    "                   properties={\n",
    "                       'annotators': 'sentiment',\n",
    "                       'outputFormat': 'json',\n",
    "                       'timeout': 10000,\n",
    "                   })\n",
    "    try:\n",
    "        for s in res[\"sentences\"]:\n",
    "            sentimentValue = int(s[\"sentimentValue\"])\n",
    "            sentiment = s[\"sentiment\"]\n",
    "            \n",
    "            # negative sentiment\n",
    "            if sentimentValue < 2:\n",
    "                totalNegativeCount = totalNegativeCount + 1\n",
    "            elif sentimentValue == 2:\n",
    "                totalNeutralCount = totalNeutralCount + 1\n",
    "            else:\n",
    "                totalPositiveCount = totalPositiveCount + 1\n",
    "                    \n",
    "            totalSentenceCount = totalSentenceCount + 1\n",
    "                \n",
    "            if totalSentenceCount > 0:\n",
    "                averagePositiveScore = totalPositiveCount/totalSentenceCount\n",
    "                averageNegativeScore = totalNegativeCount/totalSentenceCount\n",
    "                averageNeutralScroe = totalNeutralCount/totalSentenceCount\n",
    "    except:\n",
    "        print (\"Error encountered!\")\n",
    "        \n",
    "    return (averageNegativeScore, averageNeutralScroe, averagePositiveScore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n0.0 1.0 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n1.0 0.0 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n0.0 1.0 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n1.0 0.0 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n1.0 0.0 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n1.0 0.0 0.0\n2\n0.0 1.0 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n0.0 1.0 0.0\n2\n0.0 1.0 0.0\n1\n0.3333333333333333 0.6666666666666666 0.0\n2\n0.25 0.75 0.0\n1\n0.4 0.6 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n1.0 0.0 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n1.0 0.0 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n1.0 0.0 0.0\n1\n1.0 0.0 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n1.0 0.0 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n1.0 0.0 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n1.0 0.0 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n1.0 0.0 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n1.0 0.0 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n1.0 0.0 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n1.0 0.0 0.0\n1\n1.0 0.0 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n1.0 0.0 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n1.0 0.0 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n0.0 1.0 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n1.0 0.0 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n1.0 0.0 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n1.0 0.0 0.0\n1\n1.0 0.0 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Documents\\Education\\GATech\\BigDataForHealth\\Projects\\venv\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    376\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m                 \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-3c137b0a50db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstanfordSentimental_Analysis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreview_va_stop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mStanfordAnalysis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\Education\\GATech\\BigDataForHealth\\Projects\\venv\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   2996\u001b[0m         \"\"\"\n\u001b[0;32m   2997\u001b[0m         new_values = super(Series, self)._map_values(\n\u001b[1;32m-> 2998\u001b[1;33m             arg, na_action=na_action)\n\u001b[0m\u001b[0;32m   2999\u001b[0m         return self._constructor(new_values,\n\u001b[0;32m   3000\u001b[0m                                  index=self.index).__finalize__(self)\n",
      "\u001b[1;32m~\\Documents\\Education\\GATech\\BigDataForHealth\\Projects\\venv\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_map_values\u001b[1;34m(self, mapper, na_action)\u001b[0m\n\u001b[0;32m   1002\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1003\u001b[0m         \u001b[1;31m# mapper is a function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-61-3c137b0a50db>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(i)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstanfordSentimental_Analysis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreview_va_stop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mStanfordAnalysis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-60-d93dc12a0432>\u001b[0m in \u001b[0;36mStanfordAnalysis\u001b[1;34m(inputstring)\u001b[0m\n\u001b[0;32m     15\u001b[0m                        \u001b[1;34m'annotators'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'sentiment'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                        \u001b[1;34m'outputFormat'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'json'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                        \u001b[1;34m'timeout'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m                    })\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Education\\GATech\\BigDataForHealth\\Projects\\venv\\lib\\site-packages\\pycorenlp\\corenlp.py\u001b[0m in \u001b[0;36mannotate\u001b[1;34m(self, text, properties)\u001b[0m\n\u001b[0;32m     27\u001b[0m             self.server_url, params={\n\u001b[0;32m     28\u001b[0m                 \u001b[1;34m'properties'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproperties\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             }, data=data, headers={'Connection': 'close'})\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         if ('outputFormat' in properties\n",
      "\u001b[1;32m~\\Documents\\Education\\GATech\\BigDataForHealth\\Projects\\venv\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \"\"\"\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Education\\GATech\\BigDataForHealth\\Projects\\venv\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Education\\GATech\\BigDataForHealth\\Projects\\venv\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    531\u001b[0m         }\n\u001b[0;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Education\\GATech\\BigDataForHealth\\Projects\\venv\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 646\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Education\\GATech\\BigDataForHealth\\Projects\\venv\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m                 )\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Education\\GATech\\BigDataForHealth\\Projects\\venv\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m                                                   chunked=chunked)\n\u001b[0m\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m             \u001b[1;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Education\\GATech\\BigDataForHealth\\Projects\\venv\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1329\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1331\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1332\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "stanfordSentimental_Analysis = review_va_stop.map(lambda i: StanfordAnalysis(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanford_senti_neg, stanford_senti_neu, stanford_senti_pos = [list(t) for t in zip(*stanfordSentimental_Analysis)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['stanford_neg'] = stanford_senti_neg\n",
    "train['stanford_neu'] = stanford_senti_neu\n",
    "train['stanford_pos'] = stanford_senti_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StanfordAnalysisData_test = review_va_stop_test.map(lambda i: StanfordAnalysis(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanford_senti_neg_t, stanford_senti_neu_t, stanford_senti_pos_t = [list(t) for t in zip(*StanfordAnalysisData_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['stanford_neg'] = stanford_senti_neg_t\n",
    "test['stanford_neu'] = stanford_senti_neu_t\n",
    "test['stanford_pos'] = stanford_senti_pos_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53766, 14)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10565, 14)\n"
     ]
    }
   ],
   "source": [
    "test_recall_df = test.loc[test['recall_status'] == True]\n",
    "print(test_recall_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43201, 14)\n"
     ]
    }
   ],
   "source": [
    "test_nonrecall_df = test.loc[test['recall_status'] == False]\n",
    "print(test_nonrecall_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recall_df.to_csv(\"drugsTestDataRecallOnlyWithFeatures\", sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nonrecall_df.to_csv(\"drugsTestDataNonRecallOnlyWithFeatures\", sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"drugsTestingDataWithFeatures\", sep='\\t', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
